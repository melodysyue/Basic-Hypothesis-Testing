---
title: "Basic Hypothesis Testing"
author: "Yue Shi, PhD candidate, University of Washington,"
date: "4/24/2018"
output:
  github_document:
    toc: true
    toc_depth: 5
---

## Import packages and data sets

We will import ww_data file and pac_data file as a comparator, and two libraries, ggplot2 (for plotting) and BSDA (basic statistics and data analysis).
```{r message=FALSE}
library(ggplot2)
library(BSDA)
ww_data = read.table(file="http://faculty.washington.edu/dfowler/teaching/2017/GNOM560/560_ww_data.txt", header = T, sep = '\t')
pab_data = read.table(file="http://faculty.washington.edu/dfowler/teaching/2017/GNOM560/560_pab_data.txt", header = T, sep = '\t') 
```

## Parametric testing: Compare two means

#### One-sample T test
Compare sample mean to a value. For example, does sample mean deviate from $\alpha$. 

```{r}
t.test(ww_data$scaled_effect, mu=0) 
t.test(ww_data$scaled_effect, mu = 0, alternative = "two.sided") ## By default, R will do two-side t test.
t.test(ww_data$scaled_effect, mu = 0, alternative = "less")
t.test(ww_data$scaled_effect, mu = 0, alternative = "greater")
```

#### Two-sample T test

##### First, check the assumption of normality. But remember, t test is robust to non-normality.  
**Make a histogram** to check the data distribution. 
```{r message=FALSE}
ggplot(ww_data, aes(scaled_effect))+
  geom_histogram(fill="blue",alpha=0.25)

ggplot(pab_data, aes(scaled_effect))+
  geom_histogram(fill="red",alpha=0.25)
```

**Make a QQ plot**. A Q-Q plot is a graphical method for comparing two probability distributions by plotting their quantiles against each other. If the distributions are identical, the points will lie on a line with a slope of 1If they have the same shape but are related by a linear transformation, the Q-Q plot will be linear but with a slope other than 1. Q-Q plots allow you to spot outliers. QQ plot has the following format qqplot(distribution1, distribution2)

```{r message=FALSE}
qqplot(dnorm(seq(min(ww_data$scaled_effect), max(ww_data$scaled_effect), 0.01),
             mean(ww_data$scaled_effect), sd(ww_data$scaled_effect)), ww_data$scaled_effect)

qqplot(dnorm(seq(min(pab_data$scaled_effect), max(pab_data$scaled_effect), 0.01),
            mean(pab_data$scaled_effect), sd(pab_data$scaled_effect)), pab_data$scaled_effect)
```

Compare probability distributions between ww_data and pab_data. 
```{r}
qqplot(ww_data$scaled_effect, pab_data$scaled_effect)
```

**The Kolmogorov-Smirnov test** is a nonparametric test that can be used to compare a sample distribution ot a reference distribution or to compare two sample distributions. 
Try to use the Kolmogorov-Smirnov test (ks.test()) to check for normality.  As we'll learn, the KS test compares a set of data to a cumulative distribution function.  Here, you will want to use "pnorm" (the normal cumulative distribution function).
```{r warning=FALSE}
ks.test(ww_data$scaled_effect, "pnorm", mean(ww_data$scaled_effect), sd(ww_data$scaled_effect))
ks.test(pab_data$scaled_effect, "pnorm", mean(pab_data$scaled_effect), sd(pab_data$scaled_effect))
```

##### Second, check the assumption for homogeneity of variance.

**Bartlett's test** can be used to determine if samples are drawn from populations with the same variance. Try to use Bartlett's test (bartlett.test()) test to see if the variances can be pooled.  
```{r}
bartlett.test(list(ww_data$scaled_effect, pab_data$scaled_effect))
```

##### Third, do a two-sample 2-sides t test.

```{r}
t.test(ww_data$scaled_effect, pab_data$scaled_effect, paired=FALSE,var.equal=TRUE)
ggplot(ww_data, aes(scaled_effect)) + geom_density(fill = "blue", alpha = 0.25) + geom_density(data = pab_data, aes(scaled_effect), fill = "red", alpha = 0.25)
```

## Non-parametric hypothesis testing

```{r message=FALSE}
x=rnorm(100,2) #Generate some data
```

#### Sign Test
Background:  For example, when the data is not normally distributed, rather than apply a transformation, you can use sign test. It simply allocates a sign (+ or -) to each observation. It tests the equality of matched pairs of observations. The null hypotheis is that the median of the differences is zero. No further assumptions are made. **It is used in situations in which the one-sample or paired t-test might traditionally be applied**. As a rule, nonparametric methods, particularly when used in small samples, have rather less power than their parametric equivalents. 

Let's test to see if this data really does come from a distribution with a median of 2
```{r}
SIGN.test(x, md = 2) # it use median (md) instead of mean (mu). 
```

What about using one-sample t-test on this? 
```{r}
t.test(x,mu=2)
```

#### Wilconxon Signed-Rank Test 
Background: Though the sign test is extremely simple to perform, one obvious disadvantage is that it does not take the maginitude of the observation into account and may reduce the statistical power of the test. The alternative that accounts for the magnitude of the observations is the Wilcoxon signed rank test. It tests the equality of matched pairs of observations. It assumes the distribution is symmetrical. The null hypothesis is that both distributions are the same. 
```{r}
wilcox.test(x, mu = 2) #it use mean (mu) instead of median (md)
```

Now, you try with paired data. First, generate some random data from the normal distribution, calculate the difference and do the appropriate test.

```{r}
y=rnorm(100,2.5)
dif=x-y
t.test(dif,mu=0)
SIGN.test(dif,md=0)
wilcox.test(dif,mu=0)
```

**Wilcoxon signed-rank test is more powerful than the simply sign test.** Using the rank data in addition to the sign data gave us much better precision, since it has smaller p value. 
 

#### Wilcoxon Rank Sum Test (a.k.a Mann-Whitney test or Wilcoxon-Mann-Whitney test)
Background: the sign test and Wilcoxon signed rank test are useful non-parametric alternatives to the one-sample and paired t-tests. **A nonparametric alternative to the unpaired t-test is given by the Wilcoxon rank sum test, which is also known as the Mann-Whitney test.** When used in one-sample or paired test, wilcox.test means Wilcoxon signed rank test, whereas when used in unpaired two-sample test, *wilcox.test* means Wilcoxon rank sum test. 

```{r}
z=rnorm(100,2)
wilcox.test(x,z) 
```

Now make random normal data set of 1000 elements with a mean of 2 and a random gamma data set whose shape parameter is 2 (will also have an expected value of 2).  Make density plot of each, marking the means.
```{r message=FALSE}
normd=rnorm(1000, mean=2)
mean_norm=mean(normd)
gammad=rgamma(1000, shape=2)
median_gamma=median(gammad)
normd=as.data.frame(normd)
gammad=as.data.frame(gammad)
library(ggplot2)
ggplot(normd, aes(normd)) + 
  geom_density(fill = "blue", alpha = 0.25) + 
  geom_vline(xintercept=mean_norm, col="blue")+
  geom_density(data = gammad, aes(gammad), fill = "red", alpha = 0.25) +
  geom_vline(xintercept=median_gamma, col="red")
```

What do you think will happen when you do a Wilcoxon rank sum test with these data?  Give it a try and see. Remember, wilcoxon test assumes symmetric disetribution, whereas gamma distribution is not necessarily symmetrical. 
```{r}
wilcox.test(normd$normd,gammad$gammad)
```

Next, do a WRST on the WW domain reported_effect scores vs the Pab1 reported_effect scores, and compare the results with t test.
```{r}
wilcox.test(ww_data$reported_effect,pab_data$reported_effect)
t.test(ww_data$reported_effect,pab_data$reported_effect)
ggplot(ww_data, aes(reported_effect)) + 
  geom_density(fill = "blue", alpha = 0.25) + 
  geom_density(data = pab_data, aes(reported_effect), fill = "red", alpha = 0.25)
```

Since both datasets on reported effect are not normally distributed, use a non-parametrid test such as wilcoxon rank sum test is a good idea. 

#### Kruskal-Wallis Test
Background: Wilcoxon rank sum test is for comparing two independent groups, **kruskal-wallis test is an non-parametric alternative of anova for comparing three independent groups.** 

Create three random gamma-distributed data set with 100 elements and an identical shape parameter of your choice. Use kruskal.test() to verify that the medians are the same 
```{r}
d1=rgamma(100,shape=2)
d2=rgamma(100,shape=2)
d3=rgamma(100,shape=2)
kruskal.test(list(d1,d2,d3))
```

OK, now double the shape parameter for d3 and test again
```{r}
d3=rgamma(100,shape=4)
kruskal.test(list(d1,d2,d3))
```

Use **pairwise.wilcox.test()** to see which of our three data sets is different from the others (note the automatic correction for multiple hypothesis testing). It requires some formatting work.
```{r}
combined.vector=c(d1,d2,d3)
grouping.vector=c(rep("d1",length(d1)),rep("d2",length(d2)),rep("d3",length(d3)))
pairwise.wilcox.test(combined.vector,grouping.vector, paired = TRUE)
```

To get a sense of the power of the KW test, try varying the shape parameter in increments of 1% up or down and find the threshold for detectiong the difference.

```{r}
shape.param=seq(2,4,0.02)
random.gamma=sapply(shape.param,function(x){rgamma(100,x)}, simplify=FALSE)
pvalues=sapply(random.gamma, function(x){kruskal.test(list(d1,d2,x))$p.value})
par(mfrow=c(1,1))
plot(seq(from=2, to=4, by=0.02), pvalues)
abline(h=0.05,col="red")
```

Repeast this exercise 100 times. 

```{r}
random.gamma = function(x) {
shape.param = seq(2, 4, 0.02)
random.gammas = sapply(shape.param, function(x) {rgamma(100, x)}, simplify = FALSE)
pvalues = sapply(random.gammas, function(x) {kruskal.test(list(d1, d2, x))$p.value})
largest.shape = max(which(pvalues > 0.05))
return(2 + 0.02*largest.shape)
}

largest.shapes = sapply(1:100, random.gamma)

plot(density(largest.shapes), col = "red")
abline(v = mean(largest.shapes), col = "gray", lwd = 4)
```

## Resampling
There are two categories of resampling, randomization and bootstrapping. Randomization is sampling without replacement, it is for hypothesis testing. Bootstrapping is sampling with replacement, it is for parameter estimation. Resampling and non-parametric testing methods discussed above are both good for the situations when we know little about the distribution and the assumptions are violated. 

##### Sample Randomization Test

Let's examine the difference in mean outcome in a dominant model for a single SNP. First, we make up some "true" data. Here, the carrier vector is binary and specifies whether each subject either is or is not a carrier of the SNP.  The null.y and alt.y vectors give the outcome (let's say it's the expression of an associated transcript) for each subject.
```{r}
carrier = rep(c(0,1), c(100,200))
null.y = rnorm(300) #by default, mean=0, sd=1.
alt.y = rnorm(300, mean=carrier/2)
## Note that rnorm() will take a vector of means - the random value returned for the ith element will be drawn from a normal distribution with the ith mean.  Here, the first 100 values will be drawn from a normal with a mean of zero whereas the next 200 will be drawn from a normal with a mean of 0.5, note carrier/2.
```

Just for the sake of comparison, deploy the appropriate parametric test for the difference in means.
```{r}
t.test(null.y~carrier, var.equal=TRUE)
t.test(alt.y~carrier, var.equal=TRUE)
```

Now, define test statistics for randomization tests for the null and alternate distributions 
```{r}
null.diff = mean(null.y[carrier==1])-mean(null.y[carrier==0])
alt.diff = mean(alt.y[carrier==1])-mean(alt.y[carrier==0])

```

Generate 1000 random test statistics from the data.  
```{r}
random.test = function(x,y) {
sp = sample(x, replace=FALSE) # By default, it is without replacement. 
mean(y[sp==1])-mean(y[sp==0])
}

null.rand = replicate(1000, random.test(carrier, null.y))
alt.rand = replicate(1000, random.test(carrier, alt.y))
```

Plot your results and compare p-values for each distribution.
```{r}
par(mfrow = c(1,2))
hist(null.rand, xlim=c(-0.6,0.6))
abline(v=null.diff, lwd=2, col="red")
hist(alt.rand, xlim=c(-0.6,0.6))
abline(v=alt.diff, lwd=2, col="red")

mean(abs(null.rand) > abs(null.diff))
#take absolute values, since you can do .rand-.diff, or .diff-.rand. 
mean(abs(alt.rand) > abs(alt.diff))

```

Results of t-tests and randomization tests are very comparable, which is expected. Since the data is sampled from a normal distribution with equal variance and the sample size is very big. Both assumptions of t.test are met. Rampling is nearly as powerful as parametric tests. 

##### Bootstrapping

The bootstrap can be used to obtain the variance of a test statistic. Let's say we wish to set a 95% CI on the mean of our alternate hypothesis distribution (alt.y) from above.  First, create a boostrapped distribution of the mean.
```{r}
bp.mean = function (x) {
sp = sample(x, replace = TRUE)
mean(sp)
}
sp.means = replicate(1000, bp.mean(alt.y))
```

Next, find the variance of the boostrapped values without using the var() function. Compare your results to var().
```{r}
sumsqr = sum((sp.means-mean(sp.means))^2)
variance = sumsqr* (1/(length(sp.means)-1))
variance
var(sp.means)
```

Now, calculate 95% CIs for the sample mean using this value and make a histogram showing where they fall. And compare this to the percentile method for 95% CIs we learned during lecture (97.5% and 2.5%). 
```{r}
CI_upper = mean(alt.y) + 1.96*sqrt(variance)
CI_lower = mean(alt.y) - 1.96*sqrt(variance)
CI_upper_percentile = sort(sp.means)[975]
CI_lower_percentile = sort(sp.means)[25]

par(mfrow = c(1,1))
hist(sp.means, col = "gray", breaks = 20)
abline(v = CI_upper, col = "red")
abline(v = CI_lower, col = "red")
abline(v = CI_upper_percentile, col = "blue")
abline(v = CI_lower_percentile, col = "blue")
```

Pretty close, as you can see.  

Now let's repeat on a skewed distribution. First, we make up some skewed data.
```{r}
skewed.data = rexp(15, 0.1)
```

Next, we create a boostrapped distribution of the mean.
```{r}
sp.means.skewed = replicate(1000, bp.mean(skewed.data))
```

Next, find the variance of the boostrapped values without using the var() function.  Compare your results to var().
```{r}
sumsqr = sum((sp.means.skewed-mean(sp.means.skewed))^2)
variance = sumsqr* (1/(length(sp.means.skewed)-1))
variance
var(sp.means.skewed)
```

Now, calculate 95% CIs for the sample mean using this value and make a histogram showing where they fall. And then compare this to the percentile method for 95% CIs we learned during lecture.
```{r}
CI_upper = mean(skewed.data) + 1.96*sqrt(variance)
CI_lower = mean(skewed.data) - 1.96*sqrt(variance)
CI_upper_percentile = sort(sp.means.skewed)[975]
CI_lower_percentile = sort(sp.means.skewed)[25]

par(mfrow=c(1,1))
hist(sp.means.skewed, col = "gray", breaks = 20)
abline(v = CI_upper, col = "red")
abline(v = CI_lower, col = "red")
abline(v = CI_upper_percentile, col = "blue")
abline(v = CI_lower_percentile, col = "blue")
```

As you can see, the two methods produce very different results on a skewed distribution.  The percentile method is preferred in this case.
